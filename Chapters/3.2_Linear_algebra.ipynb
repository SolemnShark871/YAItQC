{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Part 2 Linear Algebra\n",
    "\n",
    "The second part of chapter 3 will cover linear algebra. Linear algebra is the framework for quantum computing, so it is very important to be familiar with. \n",
    "\n",
    "Like trigonometry, linear algebra underpins most of physics, engineering and computer science. Its importance is difficult to overstate. In 1939, Paul Dirac reformulated quantum mechanics using linear algebra [1]. Linear algebra is so important that this way of describing quantum mechanics is known as matrix mechanics. There are two key elements to linear algebra: vectors and matrices. \n",
    "\n",
    "### 3.2.1 Vectors\n",
    "\n",
    "Vectors are objects that have both direction & magnitude. Velocity is a vector because it has a magnitude, the speed at which the object travels with, and a direction, where it's moving towards. We will use a different notation to represent a vector with a straight line and an angled bracket as $ \\ket{v} $ .\n",
    "\n",
    "Imagine a car travelling diagonally across a road. At the same time, the car is travelling along the direction of the road as well as perpendicularly across the road. It can be said that the velocity of the car has a component pointing along the road and another component across the road. \n",
    "\n",
    "What makes vectors useful to work with is the fact that a vector has components. These are the individual magnitudes of the vector in each direction. We can encode the speed of the car along the road ($v_{alon}$)and the speed of the car across the road ($v_{acro}$) in a vector with two components: \n",
    "\n",
    "$$\n",
    "\\ket{v} = \\begin{bmatrix}v_{alon} \\\\ v_{acro}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here the vector $\\ket{v}$ has two components, $v_{alon}$ & $v_{acro}$ so the vector has a dimension of 2. In 3D space, any point can be described by 3 vectors. In general, a vector can be of any dimension, with any number of components. \n",
    "\n",
    "We can write any N-dimensional vector as\n",
    "\n",
    "$$\n",
    "\\ket{v} = \\begin{bmatrix} v_0 \\\\ v_1 \\\\ \\vdots \\\\ v_{N-1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where the subscript $v_i$ indicates the i $^{th}$ component of $\\ket{v}$. \n",
    "\n",
    "*Important note:* We start counting from 0 and end at $n-1$. This convention of counting from 0 is widely used in QC and conveniently is also used in python too. \n",
    "\n",
    "### 3.2.2 Adding vectors\n",
    "\n",
    "It can be useful to add vectors. All this requires is adding the corresponding components of each vector. Adding two vectors, $\\ket{a}$ & $\\ket{b}$\n",
    "\n",
    "$$\\ket{a} + \\ket{b} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_{n-1} \\end{bmatrix} + \\begin{bmatrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_{n-1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The same can be done in reverse, a vector can be split into different components. For example \n",
    "\n",
    "$$\n",
    "\\ket{a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be useful for expressing a vector in terms of its components. \n",
    "\n",
    "### 3.2.3 Multiplying a vector by a scalar \n",
    "\n",
    "Vectors can be made smaller or bigger by enlarging them by some scale factor. To make a vector bigger by some scale factor, the vector is multiplied by a scalar. To scale the vector $\\ket{v}$ by some scalar quantity, $a$, this would be done by multiplying each component of the vector by the scalar factor. \n",
    "\n",
    "$$\n",
    "a \\ket{v} = a\\begin{bmatrix} v_0 \\\\ v_1 \\\\ \\vdots \\\\ v_{n-1} \\end{bmatrix} = \\begin{bmatrix} a \\times v_0 \\\\ a \\times v_1 \\\\ \\vdots \\\\ a \\times v_{n-1} \\end{bmatrix} \n",
    "$$\n",
    "\n",
    ">### Exercise 3.3\n",
    ">\n",
    ">Imagine the vector $\\ket{v}$ given by \n",
    ">\n",
    ">$$\\ket{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$$\n",
    ">\n",
    ">i. What is the $0^{th}$ component of $\\ket{v}$ ? What about the $1^{st}$ component?\n",
    ">\n",
    ">ii. Write $\\ket{v}$ in the form \n",
    ">\n",
    ">$$\n",
    ">\\ket{v} = a\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + b \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    ">$$\n",
    ">\n",
    ">iii. Write down the vector $5\\ket{v}$ and describe what has changed to the vector\n",
    "\n",
    "\n",
    "### 3.2.4 Bra: Row vectors with a complex twist\n",
    "\n",
    "\n",
    "Another important quantity to define is the row vector version of the state vector. This is called a __bra__ and is represented by $\\bra{\\psi}$. The big difference between normal row vectors and bras is that we always take the complex conjugate for every element of the column vector (ket). \n",
    "\n",
    "For instance, the state with ket\n",
    "$$\\ket{\\psi} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ i \\end{bmatrix} $$\n",
    "\n",
    "Would have a bra given by \n",
    "\n",
    "$$\n",
    "\\bra{\\psi} = (\\ket{\\psi}^*)^T \n",
    "$$\n",
    "\n",
    "First, all the imaginary numbers in $\\ket{\\psi}$ get a minus sign in front \n",
    "\n",
    " $$= \\frac{1}{\\sqrt{2}} \\left(\\begin{bmatrix} 1 \\\\ -i \\end{bmatrix} \\right)^T $$\n",
    "\n",
    "Then we transpose, shown by the $^T$, by swapping the rows with the columns. We're left with our bra\n",
    "\n",
    "$$\\bra{\\psi} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & -i \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "### The inner product  \n",
    "\n",
    "It can be very useful to compare two vectors by measuring how aligned they are. Imagine two vectors pointing the same way. These two vectors would have a lot in common, and so would be perfectly aligned. The measure of how similar two vectors are is the inner product. The inner product can be calculated by multiplying each component of the vectors and adding them up. \n",
    "\n",
    "For example take the vectors $\\ket{a}$ & $\\ket{b}$ as \n",
    "\n",
    " $$\n",
    " \\ket{a} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} ;  \\ket{b} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} \n",
    " $$\n",
    "\n",
    " The inner product between them is $\\braket{a|b}$. This can be computed by taking the column vector $\\ket{a}$ and turning it into a row vector where every element of $\\ket{a}$ has taken the complex conjugate. \n",
    "\n",
    " $$\n",
    "\\braket{a|b} = \\begin{bmatrix} 1^* & 2^* \\end{bmatrix}  \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\n",
    " $$\n",
    "\n",
    "Here the 1st component of $\\bra{u}$ is multiplied by the 1st component of $\\ket{v}$ and the second component of of $\\bra{u}$ is multiplied by the second component of $\\ket{v}$.\n",
    "\n",
    "$$\n",
    "= (1 \\times 3) + (2 \\times 1) = 3 + 2 = 5 \n",
    "$$\n",
    "\n",
    "This gives a scalar quantity from two vectors and is used to indicate how aligned they are. The figure below shows this, where the length of $\\ket{a}$ shows the component of $\\ket{u}$ in the direction of $\\ket{v}$. This is then scaled by the length of $\\ket{v}$ to give the inner product . \n",
    "\n",
    "![Dot_product](Images/Dot_product_visualisation.png)\n",
    "\n",
    "\n",
    "The inner product depends on the angle between the vectors. If the vectors are pointing the same way, it will be bigger, if the vectors are pointing in completely opposite directions, it will be smaller. \n",
    "\n",
    "### 3.2.5 Perpendicular vectors \n",
    "\n",
    "If two vectors have no components in common, their inner product will be zero. This can be shown by two vectors that have an angle of $90\\degree$ between them.  We call these vectors perpendicular. In quantum mechanics, the preferred term is *orthogonal* which means the same thing but applies to more than just vectors. \n",
    "\n",
    "For example the vectors $\\ket{p} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ & $\\ket{q} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ have a inner product  of 0. \n",
    "\n",
    "$$ \\begin{bmatrix} 1^* & 0^* \\end{bmatrix}  \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = (1 \\times 0) + (0 \\times 1) = 0$$\n",
    "\n",
    "\n",
    "\n",
    ">### Exercise 3.4 \n",
    ">\n",
    ">Let the vector $\\ket{q}$ be given as \n",
    ">\n",
    ">$$ \\ket{q} = \\frac{1}{4} \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix} $$\n",
    ">\n",
    ">i. Using $\\ket{v}$ from the last exercise, compute \n",
    ">\n",
    ">$$ \\ket{y} = \\ket{v} + \\ket{q} $$\n",
    ">\n",
    ">ii. Compute the inner product $ \\braket{q | v} $\n",
    ">\n",
    ">iii. Compute the inner product $ \\braket{v | q} $ \n",
    ">\n",
    ">iv. Bonus question: Compare your answers for ii. & iii. Explain any differences, if any, or why there is no difference. Does this hold for all pairs of vectors?\n",
    ">\n",
    ">v. Using the result from either ii. or iii. are the vectors $\\ket{v}$ & $\\ket{q}$ perpendicular?\n",
    "\n",
    "\n",
    ">### Exercise 3.5\n",
    ">\n",
    ">Compute the inner product for the vectors \n",
    ">\n",
    ">$$ \\ket{w} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} i \\\\ 1 \\end{bmatrix} ; \\ket{z} = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} \\sqrt{2} \\\\ i \\end{bmatrix}$$\n",
    ">\n",
    ">i. $\\braket{w|z}$\n",
    ">\n",
    ">ii. $\\braket{z|w}$\n",
    ">\n",
    ">iii. Bonus question: Compare i. ii. . Is the result the same as from Exercise 3.2 iv? \n",
    ">If not, what has changed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Matrices\n",
    "\n",
    "Another object, closely related to a vector, is a matrix. A matrix is just an array of numbers which you can perform a matrix product on. All the quantum gates will be represented as matrices, so understanding how they work will be essential for quantum computations. Matrices are usually denoted by capital letters, for example $M$\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} 1 & 2  \\\\ 3 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$M$ has 4 elements: 1,2,3 & 4. \n",
    "\n",
    "### 3.3.2 Matrix-vector product \n",
    "\n",
    "As well as storing information, matrices can be used to transform vectors. This is done using the matrix-vector product. The matrix $M$ acting on the vector $\\ket{a}$ is denoted as $M\\ket{a}$. The matrix multiplication is done by multiplying the corresponding elements of $M$ & $\\ket{a}$. Starting from the first row of $M$ we multiply out the elements of $M$ & $\\ket{a}$ starting from the left and add them up. Let's call the transformed vector $\\ket{a'}$. We can compute $\\ket{a'}$ by doing the matrix-vector product of $M$ on $\\ket{a}$ as\n",
    "\n",
    "\n",
    "$$\n",
    "\\ket{a'} = M\\ket{a}\n",
    "$$\n",
    "\n",
    "Explicity calculating this gives us\n",
    "\n",
    "$$\n",
    "\\ket{a'} =  \\begin{bmatrix} 1 & 2  \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix} (1 \\times 1) + (2 \\times 2) \\\\ (3 \\times 1) + (4 \\times 2) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ket{a'} = \\begin{bmatrix} 5 \\\\ 11 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 3.3.3 Inverse matrices\n",
    "\n",
    "For many matrix-vector products, we can go back and get the original vector from the transformed vector. This means, if we know the transformed vector $\\ket{a'}$ &  the matrix $M$ we can work out the original vector $\\ket{a}$. \n",
    "\n",
    "To get the original vector, we do the matrix-vector product, but with the transformed vector and the inverse of the matrix. This gives us \n",
    "\n",
    "$$\n",
    "M^{-1}\\ket{a'} = \\ket{a}\n",
    "$$\n",
    "\n",
    "By using our equation for $\\ket{v'} = M\\ket{a}$ from earlier, this gives us \n",
    "\n",
    "$$\n",
    "M^{-1}M\\ket{a} = \\ket{a}\n",
    "$$\n",
    "\n",
    "Notice how $\\ket{a}$ is completely unchanged from applying $M$ and then $M^{-1}$. The operation $M^{-1}M$ effectively does nothing. The same would be true if we applied $M^{-1}$ first and then $M$ as $MM^{-1}$. The general name for the \"do nothing\" matrix is the identity. The identity is\n",
    "\n",
    "$$M^{-1}M = MM^{-1} = I $$\n",
    "\n",
    "As a matrix, the identity looks like \n",
    "\n",
    "$$ \n",
    "I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Before we had $\\ket{a'} = M\\ket{a}$. And now we would like to go backwards from $\\ket{a'}$ to $\\ket{a}$. This is done using the inverse of $M$: $M^{-1}$\n",
    "\n",
    "$$\n",
    " M^{-1}\\ket{a'} = \\ket{a} \n",
    "$$\n",
    "\n",
    "Notice how we can take the original equation and do the matrix multiplication by $M^{-1}$ and get the same result\n",
    "\n",
    "$$\n",
    "M^{-1}\\ket{a'} = M^{-1}M\\ket{a} = \\ket{a}\n",
    "$$\n",
    "\n",
    "$M^{-1}$ is cancelling out $M$ so that there is no effect on $\\ket{a}$. The matrix that represents this \"no effect\" is called the identity $I$ defined by \n",
    "\n",
    "\n",
    "$$ I\\ket{v} = \\ket{v}$$\n",
    "\n",
    "This gives us the important relation \n",
    "\n",
    "\n",
    "$$ M^{-1}M = MM^{-1} = I $$\n",
    "\n",
    "### 3.3.4 Unitary matrices \n",
    "\n",
    "For all quantum gates, an important property they have is that they are unitary. Unitary matrices have an inverse equal to their conjugate transpose (the dagger at the top).\n",
    "\n",
    "$$ U^\\dagger = U^{-1} $$\n",
    "\n",
    "This means \n",
    "\n",
    "$$ U^\\dagger U = U^{-1}U = I $$\n",
    "\n",
    "The conjugate transpose is the same process as it is to turn a ket (column vector) into a bra (row vector). Take You swap the rows and the columns and then replace every imaginary number with the negative version. \n",
    "\n",
    "All quantum gates are unitary matrices, it is very common to see the term unitary when talking about any quantum circuit. \n",
    "\n",
    "\n",
    "### Chapter 3 Part 2 Summary \n",
    "\n",
    "- Vectors are a collection of numbers stored in a row or column\n",
    "- Vectors can be added to each other or multiplied by a scalar\n",
    "- The inner product of two vectors tells us how aligned they are\n",
    "- Perpendicular (orthogonal) vectors have an inner product of 0\n",
    "- Matrices are arrays of numbers that can act on vectors \n",
    "- Applying a matrix to a vector turns the vector into another vector\n",
    "- Matrices can have an inverse which does the reverse of the matrix \n",
    "- Unitary matrices have an inverse equal to their conjugate transpose\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
